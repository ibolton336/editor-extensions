name: E2E Tests

on:
  workflow_call:
    inputs:
      vsix_artifact_name:
        required: true
        type: string
      trigger_context:
        required: true
        type: string # 'pr', 'release', 'manual'
    secrets:
      OPENAI_API_KEY:
        required: false

jobs:
  # Shared setup for all test jobs
  setup:
    name: Setup Test Environment
    runs-on: ubuntu-latest
    outputs:
      core_vsix: ${{ steps.set_vsix_paths.outputs.core_vsix }}
      java_vsix: ${{ steps.set_vsix_paths.outputs.java_vsix }}
      javascript_vsix: ${{ steps.set_vsix_paths.outputs.javascript_vsix }}
      go_vsix: ${{ steps.set_vsix_paths.outputs.go_vsix }}
      csharp_vsix: ${{ steps.set_vsix_paths.outputs.csharp_vsix }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version-file: ".nvmrc"
          cache: "npm"

      - name: Download VSIX artifacts
        uses: actions/download-artifact@v4
        with:
          name: ${{ inputs.vsix_artifact_name }}
          path: ./dist

      - name: Set VSIX paths
        id: set_vsix_paths
        run: |
          # Read extension names from package.json files
          CORE_NAME=$(node -p "require('./vscode/core/package.json').name")
          JAVA_NAME=$(node -p "require('./vscode/java/package.json').name")
          JS_NAME=$(node -p "require('./vscode/javascript/package.json').name")
          GO_NAME=$(node -p "require('./vscode/go/package.json').name")
          CSHARP_NAME=$(node -p "require('./vscode/csharp/package.json').name")

          # Find the actual VSIX files
          CORE_VSIX=$(ls ./dist/${CORE_NAME}-*.vsix | head -n 1)
          JAVA_VSIX=$(ls ./dist/${JAVA_NAME}-*.vsix | head -n 1)
          JS_VSIX=$(ls ./dist/${JS_NAME}-*.vsix | head -n 1)
          GO_VSIX=$(ls ./dist/${GO_NAME}-*.vsix | head -n 1)
          CSHARP_VSIX=$(ls ./dist/${CSHARP_NAME}-*.vsix | head -n 1)

          # Verify and output
          for vsix in "$CORE_VSIX" "$JAVA_VSIX" "$JS_VSIX" "$GO_VSIX" "$CSHARP_VSIX"; do
            [ ! -f "$vsix" ] && echo "Error: VSIX not found: $vsix" && exit 1
          done

          echo "core_vsix=${CORE_VSIX}" >> $GITHUB_OUTPUT
          echo "java_vsix=${JAVA_VSIX}" >> $GITHUB_OUTPUT
          echo "javascript_vsix=${JS_VSIX}" >> $GITHUB_OUTPUT
          echo "go_vsix=${GO_VSIX}" >> $GITHUB_OUTPUT
          echo "csharp_vsix=${CSHARP_VSIX}" >> $GITHUB_OUTPUT

  # Critical tests - block PRs and releases
  test-tier0:
    name: Critical Tests (@tier0)
    runs-on: ubuntu-latest
    needs: setup
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup test environment
        uses: ./.github/actions/setup-test-environment

      - name: Download VSIX artifacts
        uses: actions/download-artifact@v4
        with:
          name: ${{ inputs.vsix_artifact_name }}
          path: ./dist

      - name: Run tier0 tests (excluding slow/infrastructure/offline)
        run: npx playwright test --grep "@tier0" --grep-invert "@slow|@requires-minikube|@offline"
        working-directory: ./tests
        env:
          __TEST_EXTENSION_END_TO_END__: "true"
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          CORE_VSIX_FILE_PATH: ${{ github.workspace }}/${{ needs.setup.outputs.core_vsix }}
          JAVA_VSIX_FILE_PATH: ${{ github.workspace }}/${{ needs.setup.outputs.java_vsix }}
          JAVASCRIPT_VSIX_FILE_PATH: ${{ github.workspace }}/${{ needs.setup.outputs.javascript_vsix }}
          GO_VSIX_FILE_PATH: ${{ github.workspace }}/${{ needs.setup.outputs.go_vsix }}
          CSHARP_VSIX_FILE_PATH: ${{ github.workspace }}/${{ needs.setup.outputs.csharp_vsix }}

      # This is intentionally kept in a separate step to avoid passing in credentials
      - name: Run tier0 offline tests (no real API calls)
        run: npx playwright test --grep "@tier0" --grep "@offline"
        working-directory: ./tests
        env:
          __TEST_EXTENSION_END_TO_END__: "true"
          OPENAI_API_KEY: <dummy> # this only exists to pass provider validation notsecret
          CORE_VSIX_FILE_PATH: ${{ github.workspace }}/${{ needs.setup.outputs.core_vsix }}
          JAVA_VSIX_FILE_PATH: ${{ github.workspace }}/${{ needs.setup.outputs.java_vsix }}
          JAVASCRIPT_VSIX_FILE_PATH: ${{ github.workspace }}/${{ needs.setup.outputs.javascript_vsix }}
          GO_VSIX_FILE_PATH: ${{ github.workspace }}/${{ needs.setup.outputs.go_vsix }}
          CSHARP_VSIX_FILE_PATH: ${{ github.workspace }}/${{ needs.setup.outputs.csharp_vsix }}

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: tier0-test-results
          path: tests/test-output/
          retention-days: 7

  # Important tests - block releases only
  test-tier1:
    name: Important Tests (@tier1)
    runs-on: ubuntu-latest
    needs: setup
    # Only required for releases, but run on PRs for visibility
    continue-on-error: ${{ inputs.trigger_context == 'pr' }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup test environment
        uses: ./.github/actions/setup-test-environment

      - name: Download VSIX artifacts
        uses: actions/download-artifact@v4
        with:
          name: ${{ inputs.vsix_artifact_name }}
          path: ./dist

      - name: Run tier1 tests (excluding slow/infrastructure on PRs)
        run: |
          # Check if any tier1 tests exist
          if ! npx playwright test --list --grep "@tier1" 2>&1 | grep -q "Total: 0 tests"; then
            if [ "${{ inputs.trigger_context }}" == "pr" ]; then
              npx playwright test --grep "@tier1" --grep-invert "@slow|@requires-minikube"
            else
              npx playwright test --grep "@tier1" --grep-invert "@requires-minikube"
            fi
          else
            echo "No @tier1 tests found, skipping"
          fi
        working-directory: ./tests
        env:
          __TEST_EXTENSION_END_TO_END__: "true"
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          CORE_VSIX_FILE_PATH: ${{ github.workspace }}/${{ needs.setup.outputs.core_vsix }}
          JAVA_VSIX_FILE_PATH: ${{ github.workspace }}/${{ needs.setup.outputs.java_vsix }}
          JAVASCRIPT_VSIX_FILE_PATH: ${{ github.workspace }}/${{ needs.setup.outputs.javascript_vsix }}
          GO_VSIX_FILE_PATH: ${{ github.workspace }}/${{ needs.setup.outputs.go_vsix }}
          CSHARP_VSIX_FILE_PATH: ${{ github.workspace }}/${{ needs.setup.outputs.csharp_vsix }}

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: tier1-test-results
          path: tests/test-output/
          retention-days: 7

  # Nice-to-have tests - never block
  test-tier2:
    name: Nice-to-have Tests (@tier2)
    runs-on: ubuntu-latest
    needs: setup
    continue-on-error: true
    if: false
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup test environment
        uses: ./.github/actions/setup-test-environment

      - name: Download VSIX artifacts
        uses: actions/download-artifact@v4
        with:
          name: ${{ inputs.vsix_artifact_name }}
          path: ./dist

      - name: Run tier2 tests
        id: tier2_tests
        run: npx playwright test --grep "@tier2"
        working-directory: ./tests
        continue-on-error: true
        env:
          __TEST_EXTENSION_END_TO_END__: "true"
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          CORE_VSIX_FILE_PATH: ${{ github.workspace }}/${{ needs.setup.outputs.core_vsix }}
          JAVA_VSIX_FILE_PATH: ${{ github.workspace }}/${{ needs.setup.outputs.java_vsix }}
          JAVASCRIPT_VSIX_FILE_PATH: ${{ github.workspace }}/${{ needs.setup.outputs.javascript_vsix }}
          GO_VSIX_FILE_PATH: ${{ github.workspace }}/${{ needs.setup.outputs.go_vsix }}
          CSHARP_VSIX_FILE_PATH: ${{ github.workspace }}/${{ needs.setup.outputs.csharp_vsix }}

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: tier2-test-results
          path: tests/test-output/
          retention-days: 7

  # Experimental tests - never block
  test-tier3:
    name: Experimental Tests (@tier3)
    runs-on: ubuntu-latest
    needs: setup
    continue-on-error: true
    if: false
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup test environment
        uses: ./.github/actions/setup-test-environment

      - name: Download VSIX artifacts
        uses: actions/download-artifact@v4
        with:
          name: ${{ inputs.vsix_artifact_name }}
          path: ./dist

      - name: Run tier3 tests
        run: npx playwright test --grep "@tier3"
        working-directory: ./tests
        continue-on-error: true
        env:
          __TEST_EXTENSION_END_TO_END__: "true"
          CORE_VSIX_FILE_PATH: ${{ github.workspace }}/${{ needs.setup.outputs.core_vsix }}
          JAVA_VSIX_FILE_PATH: ${{ github.workspace }}/${{ needs.setup.outputs.java_vsix }}
          JAVASCRIPT_VSIX_FILE_PATH: ${{ github.workspace }}/${{ needs.setup.outputs.javascript_vsix }}
          GO_VSIX_FILE_PATH: ${{ github.workspace }}/${{ needs.setup.outputs.go_vsix }}
          CSHARP_VSIX_FILE_PATH: ${{ github.workspace }}/${{ needs.setup.outputs.csharp_vsix }}

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: tier3-test-results
          path: tests/test-output/
          retention-days: 7

  # Infrastructure tests - requires minikube + Konveyor
  test-infrastructure:
    name: Infrastructure Tests (@requires-minikube)
    runs-on: ubuntu-latest
    needs: setup
    # Only run on releases
    if: false
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Checkout Konveyor Operator (for scripts)
        uses: actions/checkout@v4
        with:
          repository: konveyor/operator
          path: konveyor-operator
          ref: main

      - name: Start Minikube
        uses: konveyor/operator/.github/actions/start-minikube@main
        with:
          cpus: "max"
          memory: "max"

      - name: Setup Namespace and Secrets
        run: |
          echo "=== Creating namespace and secrets ==="
          kubectl create namespace konveyor-tackle || true

          # Create API key secret for LLM proxy
          kubectl create secret generic kai-api-keys \
            --from-literal=OPENAI_API_KEY=dummy-key-for-llemulator \
            -n konveyor-tackle \
            --dry-run=client -o yaml | kubectl apply -f -

          echo "âœ“ Namespace and secrets created"

      - name: Deploy Llemulator
        run: |
          echo "=== Deploying llemulator ==="
          cd konveyor-operator
          ./test/e2e/llm-proxy/setup-llemulator.sh
          cd ..

          echo "=== Verifying llemulator deployment ==="
          kubectl get deployment llemulator -n konveyor-tackle
          kubectl get service llemulator -n konveyor-tackle

          echo "=== Waiting for llemulator to be ready ==="
          kubectl wait --for=condition=available --timeout=300s deployment/llemulator -n konveyor-tackle

          echo "âœ“ Llemulator deployed and ready"

      - name: Configure Llemulator with Test Responses
        run: |
          echo "=== Configuring llemulator with 50 test responses ==="

          kubectl port-forward -n konveyor-tackle svc/llemulator 8089:80 &
          PF_PID=$!

          for i in {1..30}; do
            if curl -s http://localhost:8089/_emulator/script > /dev/null 2>&1; then
              echo "Port-forward is ready"
              break
            fi
            echo "Waiting for port-forward... ($i/30)"
            sleep 2
          done

          # Build JSON array with 50 responses
          RESPONSES='['
          for i in {1..50}; do
            RESPONSES="${RESPONSES}\"LLEMULATOR response\""
            if [ $i -lt 50 ]; then
              RESPONSES="${RESPONSES},"
            fi
          done
          RESPONSES="${RESPONSES}]"

          # Configure llemulator with test responses using the correct API
          response=$(curl -s -X POST http://localhost:8089/_emulator/script \
            -H "Authorization: Bearer dummy-key-for-llemulator" \
            -H "Content-Type: application/json" \
            -d "{
              \"reset\": true,
              \"models\": [\"gpt-4o\", \"gpt-4\", \"gpt-3.5-turbo\"],
              \"responses\": ${RESPONSES}
            }")

          if [ $? -ne 0 ]; then
            echo "âœ— Failed to configure llemulator"
            echo "Response: $response"
            kill $PF_PID 2>/dev/null || true
            exit 1
          fi

          echo "âœ“ Successfully configured llemulator with 50 test responses"
          echo "Response: $response"

          # Clean up port-forward
          kill $PF_PID 2>/dev/null || true

      - name: Install Konveyor
        uses: konveyor/operator/.github/actions/install-konveyor@main
        with:
          namespace: konveyor-tackle
          tackle_cr: |
            kind: Tackle
            apiVersion: tackle.konveyor.io/v1alpha1
            metadata:
              name: tackle
            spec:
              feature_auth_required: true
              kai_solution_server_enabled: true
              kai_llm_proxy_enabled: true
              kai_llm_model: gpt-4o
              kai_llm_provider: openai
              kai_llm_baseurl: http://llemulator.konveyor-tackle.svc.cluster.local/v1
              kai_api_key_secret_name: kai-api-keys

      - name: Wait for Ingress
        id: wait_ingress
        run: |
          echo "=== Waiting for ingress to be ready ==="
          kubectl wait \
            -n konveyor-tackle \
            ingress/tackle \
            --timeout=600s \
            --for=jsonpath='{.status.loadBalancer.ingress[0]}'

          minikube_ip=$(minikube ip)
          echo "Minikube IP: ${minikube_ip}"
          echo "UI_URL=https://${minikube_ip}" >> $GITHUB_ENV

          echo "âœ“ Ingress is ready"

      - name: Configure Hub Admin Credentials
        run: |
          echo "=== Configuring Hub admin credentials ==="

          echo -n "Waiting for admin user in tackle realm..."
          ADMIN_EXISTS=false
          ADMIN_SECRET=$(kubectl get secret tackle-keycloak-sso -n konveyor-tackle -o jsonpath='{.data.password}' 2>/dev/null | base64 -d || true)

          for i in $(seq 1 30); do
            # Get admin token
            ADMIN_TOKEN_RESPONSE=$(kubectl exec -n konveyor-tackle deployment/tackle-hub -- curl -s -X POST \
              http://tackle-keycloak-sso:8080/auth/realms/master/protocol/openid-connect/token \
              -H "Content-Type: application/x-www-form-urlencoded" \
              -d "grant_type=password&client_id=admin-cli&username=admin&password=$ADMIN_SECRET" 2>/dev/null || echo "{}")

            if echo "$ADMIN_TOKEN_RESPONSE" | grep -q "access_token"; then
              ADMIN_TOKEN=$(echo "$ADMIN_TOKEN_RESPONSE" | jq -r '.access_token' 2>/dev/null || true)
              if [ -n "$ADMIN_TOKEN" ]; then
                # Check for admin user in tackle realm
                USERS_RESPONSE=$(kubectl exec -n konveyor-tackle deployment/tackle-hub -- curl -s \
                  "http://tackle-keycloak-sso:8080/auth/admin/realms/tackle/users?username=admin" \
                  -H "Authorization: Bearer $ADMIN_TOKEN" 2>/dev/null || echo "[]")

                if echo "$USERS_RESPONSE" | grep -q '"username":"admin"'; then
                  echo " found"
                  ADMIN_EXISTS=true

                  # Get admin user ID
                  ADMIN_USER_ID=$(echo "$USERS_RESPONSE" | jq -r '.[0].id // empty' 2>/dev/null || true)
                  break
                fi
              fi
            fi
            echo -n "."
            sleep 5
          done

          if [ "$ADMIN_EXISTS" != true ]; then
            echo " timeout (admin user may not exist yet)"
            exit 1
          fi

          echo "Got Keycloak admin token and user ID: $ADMIN_USER_ID"

          # Clear required actions
          kubectl exec -n konveyor-tackle deployment/tackle-hub -- curl -s -X PUT \
            "http://tackle-keycloak-sso:8080/auth/admin/realms/tackle/users/$ADMIN_USER_ID" \
            -H "Authorization: Bearer $ADMIN_TOKEN" \
            -H "Content-Type: application/json" \
            -d '{"requiredActions": []}' &>/dev/null

          # Reset password
          kubectl exec -n konveyor-tackle deployment/tackle-hub -- curl -s -X PUT \
            "http://tackle-keycloak-sso:8080/auth/admin/realms/tackle/users/$ADMIN_USER_ID/reset-password" \
            -H "Authorization: Bearer $ADMIN_TOKEN" \
            -H "Content-Type: application/json" \
            -d '{"type": "password", "value": "Passw0rd!", "temporary": false}' &>/dev/null

          echo "âœ“ Admin user configured (admin:Passw0rd!)"

      - name: Seed Hub with Test Data
        run: |
          echo "=== Seeding Hub with test data ==="
          chmod +x ./tests/scripts/seed-hub.sh
          HUB_URL="${{ env.UI_URL }}" \
          USERNAME="admin" \
          PASSWORD="Passw0rd!" \
          ./tests/scripts/seed-hub.sh

      - name: Check LLM Proxy and Solution Server deployments
        run: |
          echo "=== Checking if deployments exist ==="
          for i in {1..60}; do
            if kubectl get deployment llm-proxy -n konveyor-tackle > /dev/null 2>&1; then
              echo "âœ“ llm-proxy deployment found"
              break
            fi
            echo "Waiting for llm-proxy deployment to be created... ($i/60)"
            sleep 5
          done

          echo "=== Waiting for deployments to be ready ==="
          kubectl wait --for=condition=available --timeout=600s deployment/llm-proxy -n konveyor-tackle
          kubectl wait --for=condition=available --timeout=600s deployment/kai-api -n konveyor-tackle

          echo "=== Deployment status ==="
          kubectl get deployments -n konveyor-tackle | grep -E 'llm-proxy|kai-api|kai-db|llemulator'
          kubectl get pods -n konveyor-tackle | grep -E 'llm-proxy|kai-api|kai-db|llemulator'

          echo "âœ“ LLM proxy and solution server enabled"

      - name: Verify Llemulator Integration
        run: |
          echo "=== Testing llemulator integration ==="

          # Port-forward to llemulator
          kubectl port-forward -n konveyor-tackle svc/llemulator 8089:80 &
          PF_PID=$!

          # Wait for port-forward
          for i in {1..30}; do
            if curl -s http://localhost:8089/v1/chat/completions > /dev/null 2>&1; then
              echo "Port-forward is ready"
              break
            fi
            echo "Waiting for port-forward... ($i/30)"
            sleep 2
          done

          # Test llemulator with 5 requests
          echo "Testing llemulator endpoint with 5 requests..."
          FOUND=false

          for i in {1..5}; do
            RESPONSE=$(curl -s -X POST http://localhost:8089/v1/chat/completions \
              -H "Content-Type: application/json" \
              -H "Authorization: Bearer dummy-key-for-llemulator" \
              -d '{
                "model": "gpt-4o",
                "messages": [{"role": "user", "content": "test"}]
              }')

            echo "Llemulator response $i: $RESPONSE"

            # Check if this response contains our test string
            if echo "$RESPONSE" | grep -q "LLEMULATOR response"; then
              echo "âœ“ Found 'LLEMULATOR response' in response $i"
              FOUND=true
            fi
          done

          # Verify at least one response contained expected text
          if [ "$FOUND" = true ]; then
            echo "âœ“ Llemulator is returning configured responses"
          else
            echo "âœ— None of the 5 responses contained 'LLEMULATOR response'"
            kill $PF_PID 2>/dev/null || true
            exit 1
          fi

          kill $PF_PID 2>/dev/null || true
          echo "âœ“ Llemulator integration test passed"

      - name: Verify LLM Proxy Health
        run: |
          echo "=== Checking LLM proxy health ==="

          # Check llm-proxy logs
          echo "LLM Proxy logs (last 50 lines):"
          kubectl logs -n konveyor-tackle deployment/llm-proxy --tail=50

          # Check if llm-proxy is healthy
          POD=$(kubectl get pods -n konveyor-tackle -l app.kubernetes.io/name=llm-proxy -o jsonpath='{.items[0].metadata.name}')
          if [ -z "$POD" ]; then
            echo "âœ— No llm-proxy pod found"
            exit 1
          fi

          echo "LLM Proxy pod: $POD"
          kubectl get pod $POD -n konveyor-tackle

          echo "âœ“ LLM proxy is running"

      - name: Setup test environment
        uses: ./.github/actions/setup-test-environment

      - name: Download VSIX artifacts
        uses: actions/download-artifact@v4
        with:
          name: ${{ inputs.vsix_artifact_name }}
          path: ./dist

      - name: Run infrastructure tests
        run: npx playwright test --grep "@requires-minikube"
        working-directory: ./tests
        env:
          __TEST_EXTENSION_END_TO_END__: "true"
          CORE_VSIX_FILE_PATH: ${{ github.workspace }}/${{ needs.setup.outputs.core_vsix }}
          JAVA_VSIX_FILE_PATH: ${{ github.workspace }}/${{ needs.setup.outputs.java_vsix }}
          JAVASCRIPT_VSIX_FILE_PATH: ${{ github.workspace }}/${{ needs.setup.outputs.javascript_vsix }}
          GO_VSIX_FILE_PATH: ${{ github.workspace }}/${{ needs.setup.outputs.go_vsix }}
          CSHARP_VSIX_FILE_PATH: ${{ github.workspace }}/${{ needs.setup.outputs.csharp_vsix }}
          TEST_HUB_URL: ${{ env.UI_URL }}
          TEST_HUB_AUTH_ENABLED: "true"
          TEST_HUB_USERNAME: "admin"
          TEST_HUB_PASSWORD: "Passw0rd!"

      - name: Collect infrastructure logs
        if: always()
        run: |
          echo "=== Collecting infrastructure logs and diagnostics ==="
          mkdir -p tests/test-output/k8s-logs

          # Collect deployment information
          echo "Collecting deployment status..."
          kubectl get deployments -n konveyor-tackle -o wide > tests/test-output/k8s-logs/deployments.txt || true
          kubectl get pods -n konveyor-tackle -o wide > tests/test-output/k8s-logs/pods.txt || true
          kubectl get services -n konveyor-tackle -o wide > tests/test-output/k8s-logs/services.txt || true
          kubectl get ingress -n konveyor-tackle -o wide > tests/test-output/k8s-logs/ingress.txt || true

          # Collect Tackle CR configuration
          echo "Collecting Tackle CR configuration..."
          kubectl get tackle tackle -n konveyor-tackle -o yaml > tests/test-output/k8s-logs/tackle-cr.yaml || true

          # Collect pod logs
          echo "Collecting pod logs..."
          kubectl logs -n konveyor-tackle deployment/tackle-hub --tail=1000 > tests/test-output/k8s-logs/hub.log || true
          kubectl logs -n konveyor-tackle -l app=llemulator --tail=1000 > tests/test-output/k8s-logs/llemulator.log || true
          kubectl logs -n konveyor-tackle deployment/llm-proxy --tail=1000 > tests/test-output/k8s-logs/llm-proxy.log || true
          kubectl logs -n konveyor-tackle deployment/kai-api --tail=1000 > tests/test-output/k8s-logs/kai-api.log || true
          kubectl logs -n konveyor-tackle deployment/kai-db --tail=1000 > tests/test-output/k8s-logs/kai-db.log || true

          # Collect pod descriptions for debugging
          echo "Collecting pod descriptions..."
          kubectl describe pods -n konveyor-tackle > tests/test-output/k8s-logs/pods-describe.txt || true

          # Collect events
          echo "Collecting events..."
          kubectl get events -n konveyor-tackle --sort-by='.lastTimestamp' > tests/test-output/k8s-logs/events.txt || true

          # Collect secret status (not values, just existence)
          echo "Collecting secret status..."
          kubectl get secrets -n konveyor-tackle > tests/test-output/k8s-logs/secrets.txt || true

          # Test llemulator connectivity
          echo "Testing llemulator connectivity..."
          {
            kubectl port-forward -n konveyor-tackle svc/llemulator 8089:80 &
            PF_PID=$!
            sleep 5
            curl -v http://localhost:8089/v1/chat/completions \
              -H "Content-Type: application/json" \
              -H "Authorization: Bearer dummy-key-for-llemulator" \
              -d '{"model":"gpt-4o","messages":[{"role":"user","content":"test"}]}' \
              2>&1 || true
            kill $PF_PID 2>/dev/null || true
          } > tests/test-output/k8s-logs/llemulator-connectivity-test.log 2>&1

          echo "âœ“ Infrastructure logs collected"

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: infrastructure-test-results
          path: tests/test-output/
          retention-days: 1
      - name: Upload data dir
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: data-dir
          path: tests/test-data-dir/
          retention-days: 1

  # Summary job
  test-summary:
    name: Test Summary
    runs-on: ubuntu-latest
    needs: [test-tier0, test-tier1, test-tier2, test-tier3, test-infrastructure]
    if: always()
    steps:
      - name: Generate summary
        run: |
          cat >> $GITHUB_STEP_SUMMARY << 'EOF'
          ## E2E Test Results Summary

          | Tier | Status | Blocks PRs | Blocks Releases | Notes |
          |------|--------|------------|-----------------|-------|
          | @tier0 (Critical) | ${{ needs.test-tier0.result }} | âœ… | âœ… | Core functionality |
          | @tier1 (Important) | ${{ needs.test-tier1.result }} | âŒ | âœ… | Production features |
          | @tier2 (Nice-to-have) | ${{ needs.test-tier2.result }} | âŒ | âŒ | Stable validation |
          | @tier3 (Experimental) | ${{ needs.test-tier3.result }} | âŒ | âŒ | Flaky/unstable |
          | Infrastructure (@requires-minikube) | ${{ needs.test-infrastructure.result }} | âŒ | âœ… | Skipped on PRs |

          **Context:** ${{ inputs.trigger_context }}
          **Triggered by:** ${{ github.event_name }}

          ðŸ“Š [Full workflow run](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})
          EOF
